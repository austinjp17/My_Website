---
title: "Hierarchical Model Notes"
author: "Austin Palmer"
date: "2023-04-19"
output:
  html_document:
    code_folding: hide
    toc: TRUE
    toc_float: TRUE
    css: ["./styles/style.css", "./styles/code.css", "./styles/toc.css",]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```



# **Hierarchical Data**

The Cherry Blossom data presents us with a new challenge: **hierarchical data**. The top layer of this hierarchy is the population of **all** runners, not just those in our sample. In the next layer are our 36 runners, sampled from this population. Prior, this is where we've stopped. Yet in the `running` data, the final layer of the hierarchy holds the multiple observations per runner, referred to as **repeated measures** or **longitudinal data**.

Our existing Bayesian modeling toolbox presents two approaches to analyze grouped data. We can ignore the grouping structure entirely, lump all groups together, and assume that one model is appropriately universal through **complete pooling**. Or we can *separately* analyze each group and assume that one group's model doesn't tell us anything about another's through **no pooling**. In between these "no room for individuality" and "every person for themselves", **hierarchical models** offer a welcome middle ground through **partial pooling**. In general, these models embrace the following idea: though each group is unique, having been sampled from the same population, all groups are connected and thus might contain valuable information about one another.


Hierarchical, or *grouped*, data structures are common in practice. We could have data on test scores grouped by school, or product ratings grouped by consumer.

There are good reasons to collect data in this grouped manner. Data collection is often easier and more achievable and it provides insights into two important features:

  1)  **Within-group variability**:The degree of variability among multiple observations *within* each group can be interesting on its own. For example, we can examine how *consistent* an *individual's* running times are from year to year
  
  2)  **Between-group Variability**: The variability group to group. For example, we can examine the degree to which running patterns *vary* from individual to individual


###### Hierarchical Data Examples

  -   Sampled group of schools and data $y$ on multiple individual students within each school
  
  -   Sampled group of labs and data $y$ from multiple individual experiments within each lab
  
  -   Sampled group of people on whom we make multiple individual observations $y$ over time


## Why Hierarchical

**Ignoring** this type of underlying grouping structure violates the assumption of **independent** data behind the previous poisson and normal models and, in turn, can produce misleading conclusions. However, before pivoting to Hierarchical models, it's crucial to understand **why** we'd ever want to. This section will address this.

First we'll pull in the packages we need and load the data:

```{r message=FALSE, warning=FALSE, fold=FALSE}
# Load packages
library(bayesrules)
library(tidyverse)
library(rstanarm)
library(broom.mixed)
library(bayesplot)
library(forcats)
library(tidybayes)
#Load data
data(cherry_blossom_sample)
running <- cherry_blossom_sample %>% 
  select(runner, age, net)
nrow(running)

```



This data, a subset of the `Cherry` data in the **mdsr** package contains the `net` running times in minutes for 36 participants in the annual 10-mile Cherry Blossom race held in Washington, D.C.. Each runner is in their 50s and 60s and has entered the race multiple years. The plot below illustrates the degree to which some runners are faster than others, as well as the runners time variability from year to year.

```{r warning=FALSE}
ggplot(running, aes(x=runner, y=net)) +
  geom_boxplot()
```

For example, runner 17 tended to have slower times with a lot of variability between the years. In contrast, runner 29 was the fastest and most consistent among the group.

Our goal is to better understand the relationship between running time and age for runners in this age group. What we'll learn is the limitations of our current normal/poisson models.

###### Goals


-   Explore the limitations of normal/poisson models under two extremes, **complete pooling** and **no pooling**

-   Examine the benefits of the **partial pooling** provided by **hierarchical Bayesian models**.

-   Focus on the big ideas and get into depth in subsequent sections


# **Race Time Analysis**

## Complete Pooling


We'll begin by examining the relationship between running time and age using a **complete pooling** technique: combine all 252 observations across our 36 runners into **one pool** of information.

<hr>

### Data Analysis

```{r warning=FALSE}
ggplot(running, aes(y=net, x=age)) +
  geom_point()

```

In doing so, notice the relationship appears weak - there's quite a bit of variability in run times at each age with no clear trend as age increases.

<hr>

### Normal Model

To better understand the relationship, we construct a familiar Normal regression model of running times by age. Letting $Y_i$ and $X_i$ denote the running time and age for the $i$th observation in the dataset, the data structure is such:

$$Y_i|\beta_0,\beta_1,\sigma \sim N(\mu_i,\sigma^2) \hspace{5mm} \text{st:} \hspace{5mm} \mu_i = \beta_0 + \beta_1X_i$$

And to simulate this **complete pooled** model we can use weakly informative priors:

```{r Normal_Model, results=FALSE}
complete_pooled_model <- stan_glm(
  net ~ age,
  data=running, family=gaussian,
  prior_intercept = normal(0,2.5,TRUE),
  prior = normal(0,2.5,TRUE),
  prior_aux = exponential(1,TRUE),
  chains=4,iter=5000*2,seed=84735
)
```

```{r}
tidy(complete_pooled_model, conf.int = TRUE, conf.level = 0.80)
```

The simulation results closely match our earlier observations of a weak relationship between running time and age. The posterior median model suggests that running times tend to increase by a mere 0.27 minutes for each year of age. And with an 80% posterior credible interval for $\beta_1$ which straddles 0 (-0.3, 0.84), this relationship is **not significant**.

<hr>

### What's wrong?

The results seem strange. Our own experience does not support the idea that we'll continue to run at the same speed. In fact, if we examine the relationship between running time and age fo each of our 36 runners (gray lines), **almost all** have gotten slower over time *and* at a more rapid rate than suggested by the complete pooled normal regression.

```{r warning=FALSE}
ggplot(running, aes(x = age, y = net, group = runner)) + 
  geom_smooth(method = "lm", se = FALSE, color = "gray", size = 0.5) + 
  geom_abline(aes(intercept = 75.2, slope = 0.268), color = "blue")
```

Though the complete pooled model (blue) does okay, it's far from universal. The general speed, and changes in speed over time, *vary* quite a bit from runner to runner. For example, runner 20 tends to run slower than average **and**, with age, is slowing down at a more rapid rate. Runner 1 has consistently run faster than the average.

```{r warning=FALSE}
# Select an example subset
examples <- running %>% 
  filter(runner %in% c("1", "20", "22"))

ggplot(examples, aes(x = age, y = net)) + 
  geom_point() + 
  facet_wrap(~ runner) + 
  geom_abline(aes(intercept = 75.2242, slope = 0.2678), 
              color = "blue")
```

With respect to these observations, our complete pooled model really misses the mark. The model lumps all observations of run time into one population or one "pool". In doing so, it makes two assumptions:

1)  Each observation is **independent** of the others (A blanket assumption behind prior normal/poisson models)

2)  Information about the individual runners is irrelevant to our model of running time vs age

We've now observed that these assumptions are inappropriate:

1)  Though the observations on one runner might be independent of those on another, the observations **within** a runner are **correlated**. That is, how fast a runner ran in their previous race tells us something about how fast they'll run in the next.

2)  With respect to the relationship between running time and age, people are inherently different.

These assumption violations had a significant consequence: our complete pooled model failed to pick up on the fact that people tend to get slower with age. Though we've explored the complete pooling drawbacks in the specific context of the Cherry Blossom race, they are true in general.

###### Drawbacks of a complete pooling approach

There are 2 main drawbacks:

  1)  We violate the assumption of independence; and in turn,

  2)  We might produce misleading conclusions about the relationship itself and it's significance.
  
<hr>

## No Pooling

\rule{10cm}{0.4pt}

Having failed with our complete pooled model, let's swing to the other extreme. Instead of lumping everyone together into one pool and ignoring any info about runners, the **no pooling** approach considers each of our $m = 36$ runners **separately**.

### Normal Model Construction

This framework also means that the no pooling approach builds a *separate* model for each runner. Specifically, let ($Y_{ij},X_{ij}$) denote the observed run times and age for runner $j$ in their $i$th race. The data structure for the Normal linear regression model of run time vs age for runner $j$ is:

$$Y_{ij}|\beta_{0j},\beta_{1j},\sigma \sim N(\mu_{ij},\sigma^2) \hspace{5mm} \text{st:} \hspace{5mm} \mu_{ij} = \beta_{0j} + \beta_{1j}X_{ij}$$

This model allows fo each runner $j$ to have a unique intercept $\beta_{0j}$ and age coefficient $\beta_{1j}$. O, in the context of running, the no pooled models reflect the ace that some people tend to be faster than others (hence the different $\beta_{0j}$) and that *changes* in speed over time aren't the same for everyone (hence the different $\beta_{1j}$). Though we won't bother actually implementing these 36 no pooled models in R, if we utilized vague priors, the resulting *individual* posterior median models would be similar to those below:

```{r warning=FALSE}
ggplot(examples, aes(x = age, y = net)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE, fullrange = TRUE) + 
  facet_wrap(~ runner) + 
  xlim(52, 62)
```

The *seems* great at first glance. The runner-specific models pick up on runner-specific trends. However, there are two significant drawbacks to the no pooling approach. First, imagine that *you* planned to run the Cherry Blossom race in each of the next few years. Based on the no pooling results for our 3 example cases, what do you anticipate your running times to be? It's impossible to predict. The no pooling approach can't help you answer this question. Since they're tailored to the 36 individuals in our sample, the resulting 36 models don't reliably extend beyond these individuals.

###### Drawbacks

There are two main drawbacks to taking a no pooling approach to analyze group structured data:

1)  We **cannot reliably generalize** or apply the group-specific no pooled models to groups outside those in our sample

2)  No pooled models assume that **one group doesn't contain relevant information about another**, and thus ignores potentially valuable information. This is especially consequential when we have a small number of observations per group.


##### Quiz

Reexamine runner 1, if they were to race again at age 62, 5 years from their most recent data point, what would you expect their `net` to be?

a)  Below 75 min

b)  Between 75 and 85 min

c)  Above 85 min

Answer: Using the no pooled model, we would say a. Runner 1's model indicates they're getting faster with age and should be under 75 min by the time they're 62. Yet this no pooled conclusion exists in a vacuum. From the other *35* runners, we've observed that *most* people tend to get slower over time. It would be unfortunate to completely ignore this information, especially since we have a mere five race sample size for runner 1. A more reasonable prediction may be option b: though they might not maintain such a steep downward trajectory, runner 1 will likely remain a fast runner with a race time between 75 and 85 minutes. Again, this would be the *reasonable* conclusion, not the conclusion we'd make if using our no pooled models alone.







# **Spotify Popularity**

Here we'll build our first hierarchical model. We'll start simply, by assuming that we have some response variable $Y$, but no predictors $X$. Consider the following data story. Let $Y$ be a song's Spotify popularity rating on a 0-100 scale. In general, the more recent plays that a song has on the platform, the higher its popularity rating. Thus, the popularity rating *doesn't* necessarily reflect a song's overall quality, long-term popularity, or popularity beyond the Spotify audience. And though it will be tough to resist asking which song features $X$ can help us *predict* ratings, we'll focus on understanding $Y$ alone. Specifically we'd like to better understand the following:

-   What's the typical popularity of a Spotify Song

-   To what extent does popularity vary from artist to artist?

-   For any single artist, how much might popularity vary from song to song?

Other than a *vague* sense that the average popularity rating is around 50, we don't have any strong prior understanding about these dynamics, and thus will utilize weakly informative priors throughout our Spotify analysis. With that lets get the data:

```{r results=FALSE}
# Load packages
library(bayesrules)
library(tidyverse)
library(rstanarm)
library(bayesplot)
library(tidybayes)
library(broom.mixed)
library(forcats)

# Load data
data(spotify)

```

The `spotify` data in the **bayesrules** package is a subset of data containing information on 350 Spotify songs.

```{r}
spotify <- spotify %>% 
  select(artist, title, popularity) %>% 
  mutate(artist = fct_reorder(artist, popularity, .fun = 'mean'))
```

Here we select only a few relevant variables and reorder the `artist` levels according to their mean song popularity using `fct_reorder()` in the **forcats** package

A snippet reveals a grouped data structure:

```{r}
head(spotify, 5)
```

Specifically, our 350 song sample is comprised of multiple songs by 44 artists, who were in turn sampled from the population of all artists that have songs on Spotify.

```{r}
artist_means <- spotify %>% 
  group_by(artist) %>% 
  summarize(count = n(), popularity = mean(popularity))
artist_means %>%
  slice(1:2, 43:44)
```

The `artist_means` data frame summarizes the number of songs observed on each artist and the respective mean popularity. Shown above are the two artists with the lowest and highest mean popularity ratings. Keep in mind we've eliminated the nearly 10% of Spotify songs which have a popularity rating of 0, each artist in our sample should be seen as a relative success story.

<hr>

## Complete Pooled Approach

As a first approach to analyze this data, we'll implement a **complete pooled model** of song popularity which *ignores* the data's grouped structure, lumping all songs together in one sample. A density plot of this pool illustrates that there's quite a bit of variability in popularity from song to song, with ratings ranging from roughly 10 to 95.

```{r}
ggplot(spotify, aes(x = popularity)) +
  geom_density()
```

Those these ratings appear somewhat left skewed, we'll make one more simplification in our complete pooling analysis by assuming Normality. Specifically, we'll utilize the following **Normal-Normal complete pooled model** with a prior for $/mu$ that reflects our weak understanding that the average popularity rating is around 50 and, independently, a weakly informative prior for $\sigma$. These prior specifications can be confirmed using `prior_summary()`.

$$ Y_{ij}|\mu,\sigma \sim N(\mu,\sigma^2)$$ $$ \mu \sim N(50, 52^2)$$ $$\sigma \sim Exp(0.048)$$

We can think of $\mu$ and $\sigma$ as **global parameters** which do not vary by artist $j$. Across *all* songs across *all* artists in the population:

-   $\mu$ = global mean

-   $\sigma$ = global stdev in popularity from song to song

To simulate the corresponding posterior, note that the complete pooled model is a simple Normal regression model in disguise. Specifically, if we sub notions $\beta_0$ for global mean $\mu$, the following model is an **intercept-only regression model** with no predictors $X$. With this in mind, we can simulate the complete pooled model using `stan_glm()` with the formula `popularity ~ 1` where the `1` specifics an "intercept-only" term.

```{r, warning=FALSE, results=FALSE}
spotify_complete_pooled <- stan_glm(
  popularity~1,
  data=spotify, family = gaussian,
  prior_intercept = normal(50,2.5,TRUE),
  prior_aux = exponential(1,TRUE),
  chains=4, iter=5000*2, seed=84735
)
```

Posterior Summary:

```{r}
complete_summary <- tidy(spotify_complete_pooled, 
                         effects = c("fixed", "aux"), 
                         conf.int = TRUE, conf.level = 0.80)
complete_summary
```

Now suppose three artists release a new song on Spotify:

-   Mia X, the artist with the lowest mean popularity in our sample (13)

-   Beyonce, an artist with nearly the highest mean popularity in our sample (70)

-   Mohsen Beats, a musical group that we *didn't* observe

Your intuition might suggest that te predicted popularity for these three artists should be *different* - our data suggests Beyonce's song will be more popular than Mia X's. Yet, recall that by lumping all songs together, our complete pooled model *ignores* any artist-specific information. As a result, the posterior predicted popularity of any new song will be the *same* for every artist, those that are in our sample and those that are not.

Plotting the posterior predictive popularity models (light blue) alongside the observed *mean* popularity levels (dark dots) for all 44 sampled artists illustrates the unfortunate consequences of this oversimplification - it treats all artists the same even though we know they're not.

```{r}
set.seed(84735)
predictions_complete <- posterior_predict(spotify_complete_pooled,
                                          newdata = artist_means)

ppc_intervals(artist_means$popularity, yrep = predictions_complete,
              prob_outer = 0.80) +
  ggplot2::scale_x_continuous(labels = artist_means$artist,
                              breaks = 1:nrow(artist_means)) +
  xaxis_text(angle = 90, hjust = 1)
```

<hr>

## Building the Hierarchical Model

### The Hierarchy

Layers:

$\hspace{5mm}$ Layer 1: $Y_{ij}|\mu_j,\sigma_y \sim$ model of how song popularity varies WITHIN artist $j$

$\hspace{5mm}$ Layer 2: $\mu_j|\mu, \sigma_\mu \sim$ model of how the typical popularity $\mu_j$ varies BETWEEN artist

$\hspace{5mm}$ Layer 3: $\mu,\sigma_y,\sigma_\mu \sim$ prior models for shared global parameters

**Layer 1** deals with the smallest unit in our grouped data: individual songs within each artist. As with the no pooled model, this first layer is group or artist specific, and thus acknowledges that song popularity $Y_{ij}$ depends in part on the artist. Specifically, **within** each artist $j$, we assume that the popularity of sings $i$ are *Normally* distributed around some mean $\mu_j$ with standard deviation $\sigma_y$

$$Y_{ij}|\mu_j,\sigma_y \sim N(\mu_j,\sigma_y^2)$$

Thus, the meanings of the Layer 1 parameters are the same as they were for the no pooled model:

-   $\mu_j$ = mean song popularity for artist $j$

-   $\sigma_y$ = **within-group variability**, i.e., stdev in popularity from song to song within each artist

Moving forward, if we were to place separate priors on each artist mean popularity parameter $\mu_j$, we'd be right back at the no pooled model.

**Layer 2** is where the hierarchical model begins to distinguish itself. Instead of assuming that one artist's popularity levels can't tell us about another, layer 2 acknowledges that our 44 sampled artists are all drawn from the same broader population of Spotify artists. Within this population, popularity varies artist to artist. We model this variability in mean popularity **between** artists by assuming that the individual mean popularity level, $\mu_j$ are *Normally* distributed around $\mu$ with standard deviation $\sigma_\mu$.

$$\mu_j|\mu,\sigma_\mu \sim N(\mu,\sigma^2_\mu)$$

Thus, we can think of the two new parameters as follows:

-   $\mu$ = The **global average** of mean song popularity $\mu_j$ across all artists $j$

-   $\sigma_\mu$ = **Between-group variability**, i.e., the stdev in mean popularity $\mu_j$ from artist to artist

A density plot of the *mean* popularity levels for our 44 artists indicates that this prior assumption is reasonable. The artists' observed sample means *do* appear to be roughly Normally distributed around some global mean:

```{r}
ggplot(artist_means, aes(x=popularity))+
  geom_density()
```

**Layer 3** is where we must specify prior models for the **global parameters** which describe and are shared across the entire population of Spotify artists: $\mu, \sigma_y,$ and $\sigma_\mu$. As usual, the Normal model provides a reasonable prior for the mean parameter $\mu$ and the Exponential provides a reasonable prior for the stdev terms $\sigma_y$ and $\sigma_\mu$.

Putting this all together, our final hierarchical model brings together the models of how individual song popularity $Y_{ij}$ varies *within* artists (Layer 1) with the model of how mean popularity levels $\mu_j$ vary between artists (Layer 2) with our prior understanding of the entire population of artists (Layer 3). The weakly informative priors are specified here and confirmed below.

Note that we again assume a baseline song popularity rating of 50.

$\hspace{5mm}Y_{ij}|\mu_j,\sigma_y \sim N(\mu_j, \sigma_y^2) \text{ st: } \mu_j = \mu + b_j$ Model of individual songs within artist $j$

$\hspace{5mm} \mu_j|\mu,\sigma_\mu \sim N(\mu,\sigma_\mu^2)$ Model of variability between artists

$\hspace{5mm} \mu \sim N(50,52^2)$ prior models on global parameters

$\hspace{5mm} \sigma_y \sim Exp(0.048)$

$\hspace{5mm} \sigma_\mu \sim Exp(1)$

The type of model has a special name: **one-way analysis of variance (ANOVA)**. We'll explore the motivation behind this nomenclature in the next section.

### Within- vs between-group variability

The models we studied prior allowed us to explore *one* source of variability, that in the individual observations $Y$ across an entire population. In contrast, ANOVA enables us to decompose the variance in $Y$ into *two* sources:

1) $\sigma_y^2$: Variance of individual observations $Y$ **within** any given group

2)  $\sigma_\mu^2$: Variance of features **between** groups

The total variance in $Y$ is the sum of the two parts:

$$Var(Y_{ij}) = \sigma^2_y + \sigma^2_\mu$$

We can also think of this breakdown proportionately:

$$ \frac{\sigma_y^2}{\sigma_\mu^2 + \sigma_y^2} = \text{proportion of Var(} Y_{ij}\text{) that can be explained by differences in the observations within each group}$$

$$ \frac{\sigma_\mu^2}{\sigma_\mu^2 + \sigma_y^2} = \text{proportion of Var(} Y_{ij}\text{) that can be explained by differences between groups}$$

Or in the context of our Spotify analysis, we can quantify how much of the *total* variability in popularity across all songs and artists (Var($Y_{ij}$)) can be explained by differences between the *songs within each artist* ($\sigma_y^2$) and differences *between the artists*($\sigma_\mu$).

<hr>

## Hierarchical Posterior

### Posterior Simulation

Notice our hierarchical Spotify model has a total of 47 parameters: 44 artist-specific parameters ($\mu_j$) and 3 global parameters (\$\mu, \sigma*y,* \sigma\mu). To MCMC simulate we will use the `stan_glmer()` function, which operates similarly to `stan_glm()`, with two small tweaks:

-   To indicate that the `artist` variable defines the group structure of our data, as opposed to it being a predictor of `popularity`, the appropriate formula here is `popularity ~ (1 | artist)`.

-The prior for $\sigma_\mu$ is specified by `prior_covariance`. For this particular model, with only one set of artist-specific parameters $\mu_j$, this is equivalent to an Exp(1) prior.

```{r Hierarchical Model, results=FALSE}
spotify_hierarchical <- stan_glmer(
  popularity ~ (1 | artist),
  data=spotify, family=gaussian,
  prior_intercept = normal(50,2.5,TRUE),
  prior_aux = exponential(1,TRUE),
  prior_covariance = decov(reg=1, conc=1, shape=1, scale=1),
  chains=4, iter=5000*2, seed=84735
)

prior_summary(spotify_hierarchical)

```

<hr>

### Posterior Analysis

MCMC diagnostics for all 47 parameters confirm tat our simulation has stabilized:

```{r eval=FALSE}
mcmc_trace(spotify_hierarchical)
mcmc_dens_overlay(spotify_hierarchical)
mcmc_acf(spotify_hierarchical)
neff_ratio(spotify_hierarchical)
rhat(spotify_hierarchical)
```

Further, a quick posterior predictive check confirms that our hierarchical model isn't *too* wrong when it comes to capturing variability in song popularity. A set of posterior simulated datasets of song popularity are consistent with the general features of the original popularity data.

```{r}
pp_check(spotify_hierarchical) + 
  xlab("popularity")
```

Due to the sheer number and type of parameters here, summarizing our posterior understanding of all these parameters will take more care than for non-hierarchical models. We'll start with the global parameters

<hr>

### Global Parameter Analysis

To zero in on and obtain posterior summaries for global params, we can apply `tidy()` to the `spotify_hierarchical` model with `effects=fixed`, where "fixed" is synonymous with "non-varying" or "global".

```{r}
tidy(spotify_hierarchical, effects = "fixed",
     conf.int = TRUE, conf.level = 0.80)
```

Per the results, there's an 80% chance that the *average* artist has a mean popularity rating between 49.3 and 55.7.

To get the posterior medians for $\sigma_y$ and $\sigma_\mu$, we can specify `effects="ran_pars"`, i.e., `par`ameters related to `ran`domness or variability:

```{r}
tidy(spotify_hierarchical, effects="ran_pars")
```

The posterior median of $\sigma_y$(`sd_Observation.Residual`) suggest that, *within* any given artist, popularity ratings tend to vary by 14 points from song to song. The *between* stdev $\sigma_\mu$(`sd_(Intercept).artist`) tends to be higher at around 15.2. Thus, the *mean* popularity rating tends to vary by 15.2 points *from artist to artist*.

These two sources of variability suggest that popularity levels among multiple songs *by the same artist* tend to ave a moderate correlation near 0.54:

```{r}
15.1^2 / (15.1^2 + 14.0^2)
```

Thinking of this another way, 54% of the variability in song popularity is explained by differences between artists, whereas 46% is explained by differences among the songs within each artist.

<hr>

### Group-Specific Parameter Analysis

Let's now turn our focus from the global features to the artist-specific features for our 44 sample artist. Recall we can write the artist-specific mean popularity levels $\mu_j$ as *tweaks* to the global mean song popularity $\mu$

$$\mu_j = \mu + b_j$$

Thus, $b_j$ describes the *difference* between artist $j$'s mean popularity and the global mean popularity. It's these tweaks that are simulated by `stan_glmer()` - each $b_j$ has a corresponding Markov chain labeled `b[(Intercept) artist:j]`. We can obtain a `tidy()` posterior summary of all $b_j$ terms using the argument `effects = "ran_vals` (i.e., `ran`dom artist-specific `val`ues)

```{r}
artist_summary <- tidy(spotify_hierarchical, effects = "ran_vals", 
                       conf.int = TRUE, conf.level = 0.80)

artist_summary %>% 
  select(level, conf.low, conf.high) %>% 
  slice(1:2, 43:44)

```

Consider `Camilo`'s $b_j$ tweak: there's an 80% chance that `Camilo`'s mean popularity rating is between 19.4 and 32.4 *above* that of the average artist. Similarly, there's an 80% chance that Mia X's popularity rating is 23.3 to 40.7 points *below* the average artist.

We can also *combine* our MCMC simularions or the global mean $\mu$ and artist tweaks $b_j$ to directly simulate posterior models for the artist specific means $\mu_j$. For artist `j`:

$$\hspace{5mm} \mu_j = \mu + b_j = \text{ (Intercept) + b[(Intercept)artist:j]}$$

The **tidybayes** package provides some tools for this task. To begin, we use `spead_draws()` to extract the `(Intercept)` and `b[(Intercept)artist:j]` values for each artist in each iteration. We then sum these two terms to define `mu_j`. This produces an 880000-row data frame which contains 20000 MCMC samples of $\mu_j$ for each of the 44 artists $j$:

```{r}
# Get MCMC chains for each mu_j
artist_chains <- spotify_hierarchical %>%
  spread_draws(`(Intercept)`, b[,artist]) %>% 
  mutate(mu_j = `(Intercept)` + b) 

# Check it out
artist_chains %>% 
  select(artist, `(Intercept)`, b, mu_j) %>% 
  head(4)
```

For example, at the fist set of chain values, `Alok`'s mean popularity (67.4) is 16.3 points *above* the average (51.1).

Next, `mean_qi()` produces posterior summaries fo each artist's mean popularity $\mu_j$, including the posterior mean and an 80% credible interval

```{r}
# Get posterior summaries for mu_j
artist_summary_scaled <- artist_chains %>% 
  select(-`(Intercept)`, -b) %>% 
  mean_qi(.width = 0.80) %>% 
  mutate(artist = fct_reorder(artist, mu_j))

# Check out the results
artist_summary_scaled %>% 
  select(artist, mu_j, .lower, .upper) %>% 
  head(4)
```

For example, with 80% posterior certainty, we can say that Alok's mean popularity rating $\mu_j$ is between 60.3 and 68.3. Plotting the 80% posterior credible intervals for all 44 artists illustrates the variability in our posterior understanding of the mean popularity levels $\mu_j$

```{r}
ggplot(artist_summary_scaled, 
       aes(x = artist, y = mu_j, ymin = .lower, ymax = .upper)) +
  geom_pointrange() +
  xaxis_text(angle = 90, hjust = 1)
```

Not only do the $\mu_j$ posteriors vary in *location*, i.e., we expect some artists to be more popular than others, they vary in *scale* - some artists' 80% posterior credible intervals are much wider than others.

`Lil Skies` and `Frank Ocean` have similar posterior mean popularity levels, however the 80% credible interval for Lil Skies is *much* wider than tat for Frank Ocean. Why?

It's important to remember not all is equal in our hierarchical data. Whereas our posterior understanding of Frank Ocean is based on 40 songs, the *most* of any artist in the dataset, we have only 3 songs for Lil Skies. Then we naturally have greater posterior certainty about Frank Ocean's popularity, and hence narrower intervals.

```{r}
artist_means %>% 
  filter(artist %in% c("Frank Ocean", "Lil Skies"))
```

<hr>

### Posterior Prediction

We now turn our attention from trends to specifics: predicting the popularity of $Y_{new,j}$ of the next *new* song released by artist $j$ on Spotify. Though the process is different, we can utilize our hierarchical model to make a prediction for *any* artist, both those that did and didn't make it into our sample.

**Observed Group: By Hand**

First consider the posterior prediction for an observed group or artist, Frank Ocean, the $j =39$th artist in our sample. The first layer on our hierarchical model holds the key in this situation: it assumes that the popularity of individual Frank Ocean songs are Normally Distributed around his own mean popularity level $\mu_j$ with stdev $\sigma_y$. Thus, to approximate te posterior predictive model for the popularity of Ocean's *next* song on Spotify, we can simulate a prediction from the Layer 1 model evaluated at each of the 20,000 MCMC parameter sets {$\mu_j^{(i)},\sigma_y^{(i)}$}:

$$ Y_{new, j}^{(1)} | \mu_j,\sigma_y \sim N(\mu_j^{(i)}, (\sigma_y^{(i)})^2)$$

The resulting predictions {$Y_{new, j}^{(1)}, Y_{new, j}^{(2)}, .... , Y_{new, j}^{(20000)}$} and corresponding posterior predictive model will reflect two sources of variability, and hence uncertainty, in the popularity of Ocean's next song:

-   **Within-group sampling variability** in $Y$, i.e., not all of Ocean's songs are equally popular

-   **Posterior Variability** in the model parameters $\mu_j$ and $\sigma_y$, i.e., the underlying mean and variability in popularity across Ocean's songs are unknown and can, themselves, vary.

To construct this set of predictions, we first obtain the Markov chain for $\mu_j$(`mu_ocean`) by summing the global mean $\mu$ `(Intercept)` and the artist adjustment (`b[(Intercept) artist:Frank_Ocean]`) We ten simulate a prediction (`y_ocean`) from the layer 1 model at each MCMC parameter set:

```{r}
# Simulate Ocean's posterior predictive model
set.seed(84735)
spotify_hierarchical_df <- as.data.frame(spotify_hierarchical)
ocean_chains <- spotify_hierarchical_df %>%
  rename(b = `b[(Intercept) artist:Frank_Ocean]`) %>% 
  select(`(Intercept)`, b, sigma) %>% 
  mutate(mu_ocean = `(Intercept)` + b,
         y_ocean = rnorm(20000, mean = mu_ocean, sd = sigma))

# Check it out
head(ocean_chains, 5)

```

We summarize Ocean's posterior predictive model using `mean_qi()` below.

```{r}
# Posterior summary of mu_j
artist_summary_scaled %>% 
  filter(artist == "artist:Frank_Ocean")

# Posterior summary of Y_new,j
ocean_chains %>% 
  mean_qi(y_ocean, .width = 0.80) 
```

There's an 80% posterior chance that Ocean's next song will enjoy a popularity rating $Y_{new,j}$ between 51.3 and 87.6. This range is *much* wider than the 80% credible interval for Ocean's $\mu_j$ parameter (66.6, 72.2). Naturally, we can be much more certain about Ocean's underlying *mean* song popularity than that of a single Ocean song.

**Unobserved Group: By Hand**

Next consider a **posterior prediction for a yet unobserved group**. No observed songs for Mohsen Beats means we do *not* have any information about their mean popularity $\mu_j$, and thus can't take the same approach. What we *do* know is this:

1)  Mohsen Beats is an artist within the broader population of artists

2)  Mean popularity levels among these artists are Normally distributed around some global mean $\mu$ with between-artist stdev $\sigma_\mu$

3)  Our 44 sampled artists have informed our posterior understanding of this broader population.

Then to approximate the posterior predictive model for the popularity of Mohsen Beats' next song, we can simulate 20,000 predictions through a two-step process:

-   **Step 1**: Simulate a potential mean popularity level $\mu_{mohsen}$ for Mohsen Beats by drawing from the Layers 2 model evaluated at each MCMC parameter set {$\mu^{(i)}, \sigma_\mu^{(i)}$}

$$\mu_{mohsen} ^{(i)} | \mu, \sigma_\mu \sim N(\mu^{(i)}, (\sigma_\mu^{(i)})^2$$

-   **Step 2**: Simulate a prediction of song popularity $Y_{new, mohsen}$ from the Layer 1 model evaluated at each MCMC parameter set {$\mu_{mohsen}^{(i)},\sigma_\mu^{(i)}$}:

$$Y_{new, mohsen}^{(i)}|\mu_{mohsen},\sigma_y \sim N(\mu_{mohsen}^{(i)}, (\sigma_\mu^{(i)})^2)$$

Thea additional step in our Mohsen Beats posterior prediction reflects a *third* source of variability. When predicting song popularity for a new group, we must account for:

-   **Within-group sampling variability** in $Y$, i.e., not all of Mohsen Beats' *songs* are equally popular

-   **between-group sampling variability** in $\mu_j$, i.e., not all *artists* are equally popular

-   **posterior variability** in the global model parameter ($\sigma_y, \mu, \sigma_\mu$)

We implement these ideas to simulate 20,000 posterior predictions for Mohsen Beats below:

```{r}
set.seed(84735)
mohsen_chains <- spotify_hierarchical_df %>%
  mutate(sigma_mu = sqrt(`Sigma[artist:(Intercept),(Intercept)]`),
         mu_mohsen = rnorm(20000, `(Intercept)`, sigma_mu),
         y_mohsen = rnorm(20000, mu_mohsen, sigma))

# Posterior predictive summaries
mohsen_chains %>% 
  mean_qi(y_mohsen, .width = 0.80)
```

Knowing *nothing* about Mohsen Beats other than that they are an artist, we're able to predict wit 80% posterior certainty that their next song will have a popularity rating somewhere between 25.72 and 78.82.

**Shortcut Prediction using `posterior_predict()`**

```{r}
set.seed(84735)
prediction_shortcut <- posterior_predict(
  spotify_hierarchical,
  newdata = data.frame(artist = c("Frank Ocean", "Mohsen Beats")))

# Posterior predictive model plots
mcmc_areas(prediction_shortcut, prob = 0.8) +
  ggplot2::scale_y_discrete(labels = c("Frank Ocean", "Mohsen Beats"))

```

<hr>

### Shrinkage and bias-variance trade-off

We now apply last sections techniques to all 44 artists in our sample:

```{r}
set.seed(84735)
predictions_hierarchical <- posterior_predict(spotify_hierarchical, 
                                              newdata = artist_means)

# Posterior predictive plots
ppc_intervals(artist_means$popularity, yrep = predictions_hierarchical, 
              prob_outer = 0.80) +
  ggplot2::scale_x_continuous(labels = artist_means$artist, 
                              breaks = 1:nrow(artist_means)) +
  xaxis_text(angle = 90, hjust = 1) + 
  geom_hline(yintercept = 58.4, linetype = "dashed")
```

In the plot we can observe a phenomenon called **shrinkage**

To understand shrinkage, we first need to remember where we've been. Our first Spotify analysis was with a complete pooled model which entirely ignored the fact that our data is grouped are artist. As a result, it's posterior mean predictions of song popularity were the same for each artist. When using weakly informative priors, this shared prediction is *roughly* equivalent to the global mean popularity levels across all $n=350$ songs in the `spotify` sample, ignoring artist:

$$\bar{y}_{global} = \frac{1}{n}\sum_{\text{all } i,j}y_{ij}$$

We then swung the other direction, using a no pooled model which separately analyzed each artist. As such, when using weakly informative priors, the no pooled predictive means were *roughly* equivalent to the same mean popularity for each artist.

$$\bar{y}_j = \frac{1}{n_j}\sum_{i=1}^{n_j}y_{ij}$$

The above plot contrasts the hierarchical model posterior mean predictions with the complete pooled model predictions (dashed horizontal line) and no pooled model predictions (dark blue dots). In general, our hierarchical posterior understanding of artists strikes a *balance* between these two extremes - the hierarchical predictions are *pulled* or *shrunk* toward the global trends of the complete pooled model and away from the local trends of the no pooled model. Hence, **shrinkage**.

  
We can see these dynamics in the above plot - some artists shrunk more toward the global mean popularity levels than others. The artists that shrunk the *most* are those with smaller sample sizes and popularity levels at the extremes of the spectrum. Consider two of the most popular artists: `Camila Cabello` and `Lil Skies`. Though Cabello's observed mean popularity is slightly lower than lil skies, it's based on a *much* bigger sample size:

```{r}
artist_means %>% 
  filter(artist %in% c("Camila Cabello", "Lil Skies"))
```

As such, Lil Skies' posterior predictive mean popularity shrunk closer to the global mean - the data on other artists in our sample suggests that `Lil Skies` might have beginner luck, and thus that their next song will likely be below their current three-song average.

###### Shrinkage

The phenomenon in which the group-specific local trends in a hierarchical model are pulled or *shrunk* toward the global trends.
When utilizing weakly informative priors, the posterior mean predictions of song popularity from the hierarchical model are (roughly) weighted averages of those from the complete pooled ($\bar{y}_{global}$) and no pooled (\bar{y}_j) models:


$$\frac{\sigma_y^2}{\sigma_y^2 + n_j\sigma^2_\mu}\bar{y}_{global} + \frac{n_j\sigma^2_\mu}{\sigma_y^2 + n_j\sigma^2_\mu}\bar{y}_j$$

In posterior predictions fo artist $j$, the *weights* given to the global and local means depend upon how much data we have on artist $j$ ($n_j$) as well as the comparison of the *within*-group and *between*-group variability in song popularity ($\sigma_y$ and $\sigma_\mu$). These weights highlight a couple scenarios in which individualism fades, i.e., our hierarchical posterior predictions shrink away from the group-specific means $\bar{y}_j$ and toward the global mean $\bar{y}_{global}$:

  - Shrinkage increases as the number of observations on group $j$, $n_j$, decreases. That is, we rely more and more on global trends to understand a group for which we have little data
  
  - Shrinkage increases when the variability within groups, $\sigma_y$, is large in comparison to between-group variability, $\sigma_\mu$. That is, we rely more and more on global trends to understand a group when there is little distinction in the patterns from one group to the next.
  
  

<style>
@import url('https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,400;0,700;0,900;1,400;1,700&display=swap');
</style>